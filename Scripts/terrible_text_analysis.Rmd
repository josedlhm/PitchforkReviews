---
title: "text_analysis"
author: "himanshi & jose"
date: "12/12/2021"
output: html_document
---

```{r setup, echo=FALSE, include=FALSE}
library(readtext)
library(tidyverse)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.corpora)
library(ggplot2)
library(purrr)
library(stringr)
library(ggridges)
library(forcats)
library(kableExtra)
```


```{r }
pitchfork <- read.csv("..Data/pitchfork_modified.csv")

```


```{r, echo=FALSE, include=T}
# creates a quanteda corpus 
pitchfork_corpus <- pitchfork %>% 
  quanteda::corpus(text_field = "content")

```

# Pre-Processing 

```{r, echo=FALSE, include=T}
# creates corpus tokens 
pitchfork_tokens <- tokens(pitchfork_corpus, 
                           remove_punct = T, 
                           remove_numbers = T, 
                           remove_symbols = T, 
                           include_docvars = TRUE) %>%
  quanteda::tokens_tolower() %>% 
  quanteda::tokens_remove(stopwords('en'), min_nchar=2)# keeps words with 2 or more characters


```

# creates document feature matrix 
```{r, echo=FALSE, include=T}
# creates a document feature matrix 
dfm <- quanteda::dfm(pitchfork_tokens)
# keeps terms that occur 20 or more times 
dfm <- dfm_trim(dfm, min_termfreq = 20)

```


# Keyness 

```{r, echo=FALSE, include=T}
# generate keyness
tstat_key <- textstat_keyness(dfm, 
                              target = dfm$terrible_music==1)

```


```{r, echo=FALSE, include=T}

terrible_music_terms <- tstat_key %>% head(30) %>% 
ggplot(aes(reorder(x=feature, chi2), y=chi2))+
  geom_bar(stat="identity", fill="purple")+
  coord_flip()+
  labs(title ="Correlation between Terrible Music and Words", 
       y ="Strength of association (Chi Squared)", 
       x= NULL)+
  theme_minimal()

terrible_music_terms

ggsave("../Images/terrible_words_1.png", 
       plot = terrible_music_terms, 
       dpi=300)


```


All but five words associated with terrible music -shat, shit, awful, unlistenable and worst-  appear to be proper nouns: names of artists, or perhaps albums, which received bad reviews. This means that, on the whole, the language used to describe bad and good music is fairly similar; what is different are just the particulars. 

Before exploring these more general terms, let's assess just how bad the named offenders are. In other words, let's see how the artists or albums appearing in this list fare in terms of review scores. 


```{r, echo=FALSE, include=T}
# gets list of terms from tstat
list_of_terms <- tstat_key$feature[1:30]
# excludes non-nouns
list_of_terms <- list_of_terms[!list_of_terms%in% c("shat",
                                                    "shit", 
                                                    "worst", 
                                                    "awful", 
                                                    "unlistenable")]

# empty vector
artist_names <- c(1:length(list_of_terms))

# gets doc with most occurences of term
for (i in 1:length(list_of_terms)) {
  occurrence <- kwic(pitchfork_tokens, 
                     list_of_terms[i], 
                     window = 5) #gets occurences of term
  
  doc_name <- occurrence %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% 
  arrange(desc(n)) %>% 
  head(1) %>% 
  pull(docname) # gets name of doc with highest occurences
  
  # converts doc name to digit
  text_digit <- as.numeric(str_extract(doc_name,"[[:digit:]]+"))
  
  #gets artist name from corpus, using doc name
  artist_names[i] <- pitchfork_corpus[text_digit]$artist.x
}

```



```{r, echo=FALSE, include=T}
terrible_artist_plot <- pitchfork %>% 
  filter(!is.na(artist.x)) %>% 
  mutate(terrible_artist = ifelse(artist.x %in% artist_names, "Yes","No")) %>% 
  ggplot(aes(x = as.factor(terrible_artist), y=score, fill= as.factor(terrible_artist))) +
  geom_boxplot()+
  labs(title = "Distribution of Scores of Terrible Artists",
       x = "Name Appears in Key Terms",
       y = "Average Score") +
  scale_y_continuous(limits = c(0, 10), breaks = c(seq(0, 10, 1))) +
  scale_fill_manual(values =c("seagreen4", "purple"))+
  coord_flip()+
  theme_minimal()+
  theme(legend.position = "none")
terrible_artist_plot


ggsave("..Images/terrible_artist_plot.png", 
       plot = terrible_artist_plot, 
       dpi=300)

```

To do so, we find the reviews in which these terms appear most often, and then find the corresponding artist or album.^[It turns out that all these names refer to artists. Some words on the list are just the artist's name, such as Liz Phair, The Warlocks or The Dears; in other cases, they refer to the names of people in bands. Molko, for example, is the name of the lead singer of Placebo.] Then, we group these artists together and compare their scores to the rest of the reviews in the dataset. This is clearly a sub-performing group. The median score of all albums put out by these artists lies below 4.3. Scores in the 75 percentile are a meager 5.8, lying below even the 25th percentile of the global dataset. In other words, the association of their names with terrible music appears to be warranted. 

However the group's median is still above the 3 or below threshold we set. As the long tails on boxplot indicate, these artists have also produced albums whose scores cover almost the whole scale: from 0 to 9.6. Clearly they do not all make bad music all the time.  Liz Phair, for instance, whose name appears three times in the list (as 'phair', 'phair's' and 'liz'), is a particularly extreme case. Although three out of the four albums she released received scores under 3 in Pitchfork -her debut even received a flat 0- "Exile in Guyville" received an astounding 9.6, putting the album in the top 1% of album scores in the dataset. 

What about the rest of the words in the list? 'Shat', 'shit', 'awful', 'unlistenable' and 'worst.' The presence of the last four makes intuitive sense. They are all adjectives that can be easily paired with bad music.  But 'shat' -the word with the highest degree of association- is a curious case. While its relation to bad music is not entirely unsurprising for obvious reasons, it is also a verb, and one in the past tense. As such, it is not necessarily the easiest word to use while judging  the quality of an album. Conceivably, perhaps, one could use it to describe how an artist generated a certain song or an album -we'd rather not get into specifics here- but this is a somewhat far-fetched and very specific use.^[Actually, as we later found, one review quotes a band describing their own discography in these terms, as "shit we have shat." Despite the low appraisal of their own work, Silkworm has fared quite well in the eyes of Pitchfork reviewers.] Put differently, 'shat' lacks the flexibility of usage that other words in the list have. It doesn't really roll off the tongue. Why then is it so highly correlated with terrible music? 

Looking more closely at usages of the word, we see that it occurs in nine different reviews; only once in eight of those and 21 times in the ninth.

```{r, echo=FALSE, include=T}
shat <- kwic(pitchfork_tokens, "shat", window = 5)

shat_occurrences <- shat %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% 
  arrange(desc(n)) 


```

The eight reviews containing single mentions of 'shat' received a collective average of 6.55, which is slightly below the 7.01 mean for the whole dataset but still substantially above the threshold for our definition of terrible music. Clearly, gastro-intenstinal associations are perhaps not what you want to evoke with your music. But in many of the reviews the word is not used to judge its quality. Since this is such a sample, we were able to look at all occurrences. In one review, for instance, the author uses the word to recount an unsavory stage performance; in another it quotes directly from the lyrics. Overall, in no case was it used to make a statement about the music itself. 

```{r, echo=FALSE, include=T}

shat_doc_names <- shat_occurrences %>% tail(8) %>% as_tibble()

shat_doc_names <- shat_doc_names$docname

shat_reviews<- c(1:length(shat_doc_names))

for (i in length(shat_doc_names)){
  text_digit_2 <- as.numeric(str_extract(shat_doc_names[i],"[[:digit:]]+"))
  shat_reviews[i] <- pitchfork_corpus[text_digit_2]$reviewid
  }

shat_df <- pitchfork %>% 
  filter(reviewid %in% shat_reviews) 

mean_shat <- mean(shat_df$score, na.rm=T)

mean_shat
```


What about that *other* album that mentions the term 21 times? Perhaps unsurprisingly, 'shat' in this particular case is being used a proper noun -as in **Shat** the artist whose album *The Cunt Chronicle* received a score of 0.02 in 2003. Given the album's exceptionally low score, in addition to the high frequency of the word, the presence of 'shat' in our list of terms makes more sense. In fact, we have an inkling that it might be solely responsible for the word's high level of association with terrible music. To check whether this is the case, we exclude *The Cunt Chronicle* review from our analysis and recalculate the correlation between words and terrible music.  After the exclusion, the term 'shat' no longer figures among the  words most strongly associated with terrible music. 

[insert graph new graph]

We notice too that 'shit' is now off the list.  As it turns out, Pitchfork writers were not as foul-mouthed as first impressions suggested. This is because the same review that featured the 'shat' 21 times also included 'shit' 20 times. Without this one outlier, neither word makes the cut. In their place, we get 'Mumford', another proper noun, and 'least', a superlative.  

What about these other words: 'awful', 'unlistenable', 'worst', and 'least'? There is a more plausible case to be made that these words actually describe terrible music in general rather than particular instances of bad music. But is this actually how the words are being used in Pitchfork reviews? 


```{r, echo=FALSE, include=T}

unlistenable <- kwic(pitchfork_tokens, "\\b(u|U)nlistenable\\b", window = 5, valuetype = c("regex"))

awful <- kwic(pitchfork_tokens, "\\b(a|A)wful\\b", window = 5, valuetype = c("regex"))

least <- kwic(pitchfork_tokens, "\\b(l|L)east\\b", window = 5, valuetype = c("regex"))

worst <- kwic(pitchfork_tokens, "\\b(w|W)orst\\b", window = 5, valuetype = c("regex"))


unlistenable_df <- pitchfork %>% 
  filter(str_detect(content, "\\b(u|U)nlistenable\\b")) 

awful_df <- pitchfork %>% 
  filter(str_detect(content, "\\b(a|A)wful\\b")) 


least_df <- pitchfork %>% 
  filter(str_detect(content, "\\b(l|L)east\\b"))

worst_df <- pitchfork %>% 
  filter(str_detect(content, "\\b(w|W)orst\\b"))


unlistenable_albums <- pitchfork %>% 
  filter(str_detect(title, "\\b(u|U)nlistenable\\b")) %>% 
  group_by(title, artist.x) %>% 
  summarize(n=n()) %>% 
  nrow()

unlistenable_artists <- pitchfork %>% 
  filter(str_detect(artist.x, "\\b(u|U)nlistenable\\b")) %>% 
  group_by(artist.x) %>% 
  summarize(n=n()) %>% 
  nrow()



awful_albums <- pitchfork %>% 
  filter(str_detect(title, "\\b(a|A)wful\\b")) %>% 
  group_by(title, artist.x) %>% 
  summarize(n=n()) %>% 
  nrow()

awful_artists <- pitchfork %>% 
  filter(str_detect(artist.x, "\\b(a|A)wful\\b")) %>% 
  group_by(artist.x) %>% 
  summarize(n=n()) %>% 
  nrow()

worst_albums <- pitchfork %>% 
  filter(str_detect(title, "\\b(w|W)orst\\b")) %>% 
           group_by(title, artist.x) %>% 
           summarize(n=n()) %>% 
           nrow()

worst_artists <- pitchfork %>% 
  filter(str_detect(artist.x, "\\b(w|W)orst\\b")) %>% 
           group_by(artist.x) %>% 
           summarize(n=n()) %>% 
           nrow()



least_albums <- pitchfork %>% 
  filter(str_detect(title, "\\b(l|L)east\\b")) %>% 
  group_by(title, artist.x) %>% 
  summarize(n=n()) %>% nrow()

least_artists <- pitchfork %>% 
  filter(str_detect(artist.x, "\\b(l|L)east\\b")) %>% 
  group_by(artist.x) %>% 
  summarize(n=n()) %>% nrow()

  
n_artists <- c(unlistenable_artists, awful_artists, least_artists, worst_artists)

n_albums <- c(unlistenable_albums, awful_albums, least_albums, worst_albums)
  

  
max_score_unlistenable <- max(unlistenable_df$score, na.rm=T)
max_score_awful <- max(awful_df$score, na.rm=T)
max_score_least <- max(least_df$score, na.rm=T)
max_score_worst <- max(worst_df$score, na.rm=T)

max_scores <- c(max_score_unlistenable, max_score_awful, max_score_least, max_score_worst)


min_score_unlistenable <- min(unlistenable_df$score, na.rm=T)
min_score_awful <- min(awful_df$score, na.rm=T)
min_score_least <- min(least_df$score, na.rm=T)
min_score_worst <- min(worst_df$score, na.rm=T)

min_scores <- c(min_score_unlistenable, min_score_awful, min_score_least, min_score_worst)



median_score_unlistenable <- median(unlistenable_df$score, na.rm=T)
median_score_awful <- median(awful_df$score, na.rm=T)
median_score_least <- median(least_df$score, na.rm=T)
median_score_worst <- median(worst_df$score, na.rm=T)

median_scores <- c(median_score_unlistenable, median_score_awful, median_score_least, median_score_worst)


n_ocurrences_unlistenable <- nrow(unlistenable)

n_ocurrences_awful <- nrow(awful)

n_ocurrences_least <- nrow(least)

n_ocurrences_worst <- nrow(worst)

occurences <- c(n_ocurrences_unlistenable, n_ocurrences_awful, n_ocurrences_least, n_ocurrences_worst) 


n_reviews_unlistenable <- unlistenable %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% nrow()

n_reviews_awful <- awful %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% nrow()
  
n_reviews_least <- least %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% nrow()

n_reviews_worst <- worst %>% 
  group_by(docname) %>% 
  summarize(n=n()) %>% nrow()
  


n_reviews <- c(n_reviews_unlistenable, n_reviews_awful, n_reviews_least, n_reviews_worst )



data_terrible <- data.frame(Word = c("unlistenable", "awful", "least", "worst"),
                   occurrences = occurences,
                   Reviews = n_reviews,
                   Min_score = min_scores,
                   Max_score = max_scores,
                   Median_score = median_scores, 
                   Artist_names = n_artists,
                   Album_names = n_albums)


data_terrible 
```


```{r, echo=FALSE, include=T}

data_terrible_manicured <- data_terrible %>% kableExtra::kable(col.names =c("Word", "Occurrences", "Number of Reviews", "Min Score", "Max Score", "Median Score", "Number of Artists Names", "Number of Album Names"), 
                                        caption = "<center><strong> Terrible Words </strong></center>", 
                                        align = "cccc", 
                                        booktabs = T) %>% 
    kableExtra::kable_paper(bootstrap_options = "striped", full_width = T)  

save_kable(data_terrible_manicured, file = "..Images/table_terrible.html")

```


To get a rough sense of when, where and how often these words appear, we count the total number of occurrences and the number of reviews in which they appear, but we also check to see if there are any artists or albums whose names include the word, lest we find ourselves with another case of 'shat'. Additionally, we find the minimum and maximum scores of reviews containing the word, as well as the median. 

The results suggest several things. On the one hand, it appears that these terms are rarely being used as proper nouns. Only 'least' forms part of an artist's name: "The Boy Least Likely To." Similarly, there are three albums that contain 'worst', but no other term appears in any other album title. ^[These albums are: "Favourite Worst Nightmare" by the Arctic Monkeys, "The Worst You Can Do is Harm" by the Long Winters, and "The Worst of the Black Box Recorder" by The Black Box Recorder. They received scores of 7.4, 7.3 and 7.3, in that order.] Very generally, this at least gives us some assurance that the words are not being used simply to name things most of the time.  Of course, this does not guarantee that they are being used descriptively, or in direct relation to the music, but it helps stave off our fear that any single album or artist could drive the correlation. Similarly, for all terms, the ratio between total occurrences and number of reviews is closer to 1 than was the case for 'shat'. Again, while we cannot discard the possibility of a few particularly bad albums driving the correlation, given the larger sample of occurrences and more even spread across reviews, this appears less likely. For now, there appear to be no glaring anomalies. 

What is striking about these results, however, is that all four terms appear across the whole specturm of scores -from 0 to 10. This suggests that despite their unambiguously negative connotations, Pitchfork writers have managed to use 'unlistenable', 'awful', 'worst', and 'least' in a wide range of contexts, many of them positive.^[The one exception in this list might be 'awful', which can be used neutrally, as in: "you made an awful lot of pasta today."] Yet, one wonders: how might a word like 'unlistenable', for example, be used within a perfect 10 review? In fact, 'unlistenable' was used in two different 10-scoring reviews, both reviews of albums by Nirvana. 

A 2011 review of a reissue of Nirvana's *Nevermind* album reads: 

*"Listening to the various sessions leading up to the one that gave us the album we know -especially the nearly unlistenable "boombox" mixes of early demos- you learn very quickly that these songs didn't arrive perfectly formed in one sustained burst of inspiration."*

Then, a 2013 review of a reissue of Nirvana's *In Utero* album echoes a similar sentiment: 

*"A by-all-reports harmonious two-week quickie session with recording engineer Steve Albini in a rural Minnesota studio would lead to months of acrimonious exchanges in the press among the band, DGC, and Albini over the purportedly unlistenable nature of the results, requests for cleaner mixes, and cruddy cassette copies leaked to radio that falsely reinforced the label’s misgivings."*

Both of these texts point at the contrast between earlier versions of the band's songs and their final products. The second review, in particular, hints at the band's reluctance to having their sound 'polished off' by the recording studio. As a band that resisted the pressures of commercial musical production, and that tried to break away from the conventions of pop music, perhaps the presence of 'unlistenable' in Nirvana reviews is to be expected. 

On the other hand, consider a review containing 'unlistenable' which received a score of 0.4, Black Rebel Motorcycle Club's *The Effects of 333.*

*"Too loud to be ambient; too polished to work in the realm of noise, the cruelest confirmation of The Effects of 333's failure to be a defiantly alienating trainwreck is that it's hardly unlistenable."* 

The problem here is the album *fails* to be unlistenable enough. Had Black Motorcycle Club made an album that was more *alienating*, as the reviewer puts it, then perhaps it would have been better received. To put it mildly, this usage of 'unlistenable' defies our expectations. But here we begin to get a sense of the patterns of language across Pitchfork reviews, a pattern that will not surprise readers of the website. The language is both extremely particular and malleable. Since these are professional music critics, they are in the business of finding the most fitting words.  If something is bad, then it can be bad in a myriad of ways, and sometimes being listenable is, in fact, a form of failure. For purposes of our analysis, this means that it then becomes harder to find general patterns: words that are being consistently used in the same way in similar contexts. ^[Speculatively, we would venture to say that finding these patterns might be easier in a website like Amazon, where laypeople are bound to resort to similar terms to describe bad music because they have no professional obligations to be imaginative with their language.]



```{r, echo=FALSE, include=T}
unlistenable_df <- unlistenable_df %>% mutate(subset = "unlistenable")

awful_df <- awful_df %>% mutate(subset = "awful")

least_df <- least_df %>% mutate(subset = "least")

worst_df <- worst_df %>% mutate(subset = "worst")

pitchfork <- pitchfork %>% mutate(subset ="global")

combined_df_1 <- rbind(unlistenable_df, awful_df) 

combined_df_2 <- rbind(worst_df, least_df) 

combined_df_3 <- rbind(combined_df_1, combined_df_2)

combined_df <- rbind(combined_df_3, pitchfork)

combined_df <- combined_df %>% mutate(subset = fct_relevel(subset, c("unlistenable", "awful", "worst", "least", "global")))

```



```{r, echo=FALSE, include=T}
ridge_plot <- ggplot(combined_df, aes(x = score, y = subset)) +
  geom_density_ridges(scale=1, aes(fill=subset)) +
   labs(title = "Distribution of Scores by Word",
       x = "Score",
       y = "Subset by Word", 
       caption="'Global' includes 18149 observations; 'Least' 4480; 'Worst' 921; 'Awful' 397 and 'Unlistenable' 111") +
  scale_x_continuous(limits = c(0, 10), breaks = c(seq(0, 10, 1))) +
  scale_y_discrete(labels=c("Unlistenable", "Awful", "Worst", "Least", "Global"))+
  scale_fill_manual(values =c("purple1", "purple2", "purple3", "purple4", "seagreen4"))+
  theme_minimal()+
  theme(legend.position = "none")

ggsave("../Images/ridge_plot_1.png", 
       plot = ridge_plot, 
       dpi=300)

```


We know then that these terms are being used in different ways, and this explains why they appear in both positive and negative reviews. However, as our initial analysis showed, these words *are* correlated with terrible reviews. To explore why this is the case, let's take a look at the distribution of scores.

The distribution plot above confirms something the table had already told us: the median scores for all of these subsets lie below but close to the global median of 7.2. While it is impossible to determine exact medians from the graph alone, the distributions' peaks indicate the points with the highest concentration of reviews; in most cases the median will lie somewhere in that neighborhood. For all these distributions, the highest point lies somewhere between 6 and 8. Crucially, however, the figure also shows that in contrast to the whole dataset -here called global- the distribution of reviews containing the four terms have thicker left tails. This means that they contain  more reviews that fall on the lower end of the score spectrum. In the distribution for  'unlistenable', for example, there is even a bump between 2.5 and 3, suggesting a concentration of reviews around that score range. This result, here shown graphically, is consistent with the association we had seen earlier. 

Broadly then, it appears that 'unlistenable' *is* generally being used to describe terrible music, at least some of the time. However, we should note that as the number of observations increases, this trend becomes less pronounced. Indeed, as a word appears in more reviews, the left tail of the distribution of scores becomes thinner, and the overall shape becomes more similar to the  global distribution. The subset for 'least', for example, which contains 4480 reviews, is the closest in appearance to the global distribution, while the subset for 'unlistenable', which contains only 111, is the most different.^[This is consistent, in a way, with the Central Limit Theorem, which stipulates that as sample size increases, the shape of a distribution approximates a bell curve. Here the approximation is not to a perfect bell curve but to Pitchfork's left-skewed distribution.] 

Given our list of words, this trend makes intuitive sense. Although we cannot prove that this is the case, we can venture a guess as to why some of these words appear more frequently, and others are more closely correlated with terrible music. 'Unlistenable' is the only acoustic term in the list. As such, it is more specific, explaining its lower frequency, but also perhaps the most likely to be used to describe bad music. 'Awful', 'worst' and 'least' are also all pejorative, but they can be used to describe a variety of things that are not necessarily musical. One can say, for example, that the album cover is awful or the worst, but one cannot say that it is unlistenable. In this sense, it is less likely that their appearance says something about the quality of the music itself. 




